\documentclass{report}

\input{commands}

%\lhead{Left Header}
%\rhead{Right Header}

\begin{document}

% Change Font and Spacing
\large % change the font size to 12pt
\linespread{1.1} % change the line spacing

% Set the counter
\setcounter{section}{0}

\tableofcontents

\chapter{Prolougue}

This set of notes discusses discrete time and continuous time Markov Processes.  The notes are compiled from lectures delivered by Mihai Sirbu at the University of Texas in the Spring of 2009.

\chapter{Markov Chains}

We begin this narrative with discrete time stochastic processes.  Psersonally, I have found that discrete time probability provides the intuition necessary to be an effective probabilist.  The theorems one encounters in discrete time stochastic processes almost always carry over to continuous time, but with slight technical modifications.  Thus the first portion of these notes will be devoted to discrete time.  We begin we some preliminary definitions.

Suppose that we have a state space $(S,\mcS)$ and a probability space $(\Omega, \mcF, \bbP)$.  A sequence of $\mcS-\mcF$ measurable random variables $(X_n)_{n \in \bbN}$ is a stochastic process.  Note that we index time in the natural numbers.  We might also index time in the whole numbers $\bbN_0$.  Sometimes we will suppress the subscript.

Statistically speaking, the law of the process is important.  This is a somewhat ambiguous statement.  Generally, the law of the process refers to the distribution generated by the finitie dimensional distributions of the law of the process.  By this I mean that for every finite subsequence $(n_k)$ of the natural numbers we look at the distribution defined by $Q^{(n)}(A_1 \times \cdots \times A_k) = \bbP(X_{n_1} \in A_1, \cdots, X_{n_k} \in A_k)$.  The collection of such distrubtions can be extended to a unique measure $\bbQ$.  This can be thought of the law of the entire process.  This law corresponds to the distribution generated by $\bbP(X \in A)$ where $A$ is a Borel set in the set of functions from $\bbN \times \Omega$ into $\bbR$.

The foregone discussion is still rather ambiguous.  We need to be more specific about the statistical properties we expect our process to possess.  For instance, we might be interested in martingales.  Statistically speaking, a martingale is a process whose forecast any distance into the future, given the entire history of the process, is the current value of the process.  Using mathematical syntax this is saying that $\bbE[X_{n+t} | X_0, \cdots, X_n ] = X_n$.  Another interpretation of this property is that the statistic composed of $X_0$ through $X_n$ that closest to $X_{n+t}$ is identical to $X_n$.  (Really, statistic here is taken in some sort of extended sense, whereby we mean any $\sigma(X_0, \cdots, X_n)$-measurable function.) 

Martingales can be thought of as the fundamental random object within stochastic processes.  All stochastic process can be decomposed into a ``deterministic'' part and a ``random'' part.  The random part is always a martingale.  Martingales can also be thought of as the largest non-parametric collection of stochastic processes (modulo processes of finite variation).  In this way, the collection of martingales is to a parametric family of random process as an $L_1$ function is to the parametric family of normal random variables.

As suggested by the title of this course, we are interested in yet more specific stochastic processes.  In particular, we want to study Markov processes.  Markov processes are a special subset of martingales.  

Whereas martingales are defined as a forecasted expected value given the historical trajectory, Markov Processes are defined in terms of future distributions conditioned on past information.  In particular, a \textbf{Markov process} \index{Markov Process} $X_n$ is a process such that the distribution of $X_{n+t}$ conditioned on $X_0, \cdots, X_n$ is identical to the distribution of $X_{n+t}$ conditioned on $X_n$.  In mathematical syntax,
\[
\bbP(X_{n+1} \in B | X_0, \cdots, X_n) = \bbP(X_{n+1} \in B | X_n) \textmd{ for all } B \in \mcB(\R^d).
\]
A Markvo process is \index{time homogeneous} time homogeneous when the conditional distribution of $X_{n+1}$ given $X_n$ does not depend on $n$.

The first consequence of this definition is that a Markov process is a martingale as claimed earlier.  In particular, a Markov process is a martingale whose forecast any distance into the future, given \emph{only its current value}, is its current value.  Interpreting this in terms of statistics again, the  statistic composed soley of $X_n$ that is closest to $X_{n+t}$ is identical to $X_n$.  We often say that ``Markov Processes'' have no memory, since the above definitions tell us that
\[
\bbE[X_{n+t} | X_0, \cdots, X_n] = \bbE[X_{n+t} | X_n ].
\]
In other words, all the information encoded in $X_0$ through $X_{n-1}$ is not used; it is forgotten.

\begin{example}[Ehrenfest Chain]

Imagine a box containing $r$ particles is partitioned by a panel with a small opening.  At each step in time, a particle is picked uniformly at random and moved to the opposite side of the box.  Thus if there are $k$ particles in Room 1 at time $n$, and a particle is selected from Room 1, then there will be $k-1$ particles in Room 1 at time $n+1$.  Conversely, if there are $k$ particles in Room 1 at time $n$, and a particle is selected form Room 2, there there will be $k+1$ particles ir Room 1 at time $n+1$.  To put this within a probabilistic framework, let
\[
X_n := \{ \# \textmd{ of particles in Room 1} \}.
\]
Since a particle was selected uniformly at random the  distribution of $X_{n+1}$ given $X_n$ is defined by
\[
\begin{cases}
\bbP(X_{n+1} = k-1 | X_n = k) = k/r \\
\bbP(X_{n+1} = k+1 | X_n = k) = (r-k)/r
\end{cases}.
\]
In this case we construct the probability $\bbP$ so that $X_n$ is a Markov Process under $\bbP$.  Throughout this course we will construct various Markov processes.  Conversely, sometimes we will be given a process and then show that it is Markov.
\end{example}

\begin{example}[Branching Process]
We construct another Markov process called a branching process.  We can think of this process as characterizing a tree or as recording a population of individuals.  Let $(\xi_i)$ be a sequence of independent and identically distributed random variables mapping into the natural numbers.  The process $X_n$ records the number of individuals in the population.  At time $n+1$, one adds $\xi_{n+1}$ individuals to the population resulting in $X_{n+1} = X_n + \xi_{n+1}$ individuals.  Thus the distribution of $X_{n+1}$ given $X_n$ is
\[
\bbP(X_{n+1} = j | X_n = i) = \bbP(\xi_{n+1} = j-i).
\]
\end{example}

\begin{example}

\end{example}

\end{document}
