\documentclass{report}

\input{commands}

%\lhead{Left Header}
%\rhead{Right Header}

\begin{document}

% Change Font and Spacing
\large % change the font size to 12pt
\linespread{1.1} % change the line spacing

% Set the counter
\setcounter{section}{0}

\tableofcontents

\chapter{Introduction and Background}

This set of notes discusses discrete time and continuous time Markov Processes.  The notes are compiled from lectures delivered by Mihai Sirbu at the University of Texas in the Spring of 2009.

\section{Conditional Probability and Conditional Expectation}

My probabilistic upbringing was a bit scattered and hence this section is an opportunity to collect my thoughts and orgainze them into a coherent whole.  Conditional probability and conditional expectation are not well defined.  By this I mean that there are several different types of conditional probabilities and conditional expectations.  The notation for these different entities is often similar, which is justified since there is a way to tie everything together.  However, it is not always obvious how one makes these connections.  To give one example, $\bbE[X | Y]$ is a different mathematical object than $\bbE[X | Y = y]$, though these two entities are related.  These conditional expectations are connected since $\bbE[X|Y] = \bbE[X|Y=y]|_{Y}$.  The notation helps and hinders simultaneously.

I hope to provide a narrative of conditional probability and conditional expectation that is intuitive.  From this perspective, it may seem, at times as if conditional expectation and conditional probability serve different purposes.  They do to some extent, however one should always remember that a probability and expectation are complimentary mathematical objects.  Given a probability $\bbP$ one can construct an expectation $\bbE$ through Catheodory's Crank.  Conversely, given an expectation $\bbE$ one can construct a probability $\bbP$ by $\bbP(A) = \bbE[1_A]$.  Thus there is always a bridge linking probabiltiy to expectation.  We begin with conditional probability since I find this to be a more fundamental concept.

\subsection{Conditional Probability}

There are several different ways to go about defining conditional expectation.  The simplest and most intuitive conditonal expectation is derived from conditional probability.  Suppose we are given a probability space $(\Omega, \mcF, \mcP)$.  Philosophically, we think of this probability space encoding all the possible eventualities one could encounter in the world.  But suppose we are interested in some subset of eventualities, $B \in \mcF$.  We can then ask what is the probabilty of some event $A \in \mcF$ occuring given that $B$ holds.  The probability of $A$ given $B$ is
\[
\bbP(A | B) = \frac{\bbP(A \cap B)}{\bbP(B)}.
\]
Essentially, we have defined a new probability for a world in which only subsets of $B$ may possible occur.  This definition makes intuitive sense.  It measures the ``amount'' of $A$ that overlaps with $B$, normalized by the ``amount'' of $B$ in $\Omega$.

Like any other probability, this can be used to construct integrals. Given a simple function $\phi = \sum a_i \1_{A_i}$, the integral of $\phi$ is
\[
\int_\Omega \phi \; \bbP(d\omega|B) = \sum a_i \bbP(A_i | B). 
\]
This can be generalized to define integrals, which we call conditional expectations and are of the form
\[
\bbE[Z|B] = \int_\Omega Z \; \bbP(d \omega | B).
\]
One encounters the idea of conditional expectation often in probability, and hence one may be predisposed to think that a conditional expecation is more useful than a conditional probability.  This is not the case.  A conditional probability encodes much more information than a conditional expectation.

The definition of conditional probability and conditional expectation may be generalized further so that one conditions on a partition of $\Omega$.  In particular, suppose we have a partition $\mcP = \{B_i\}$ of $\Omega$.  Then we can define the conditional probability and conditional expectation for each $B_i$ as above.  We can then define a new form of conditional probability and conditional expectation ``simultaneously across the entire partition'' by
\[
\bbP(A | \mcP)(\omega) = \sum_{i} \bbP(A|B_i) 1_{B_i}(\omega)
\]
and
\[
\bbE[Z | \mcP](\omega) = \sum_{i} \bbE[Z|B_i] 1_{B_i}(\omega).
\]
Before the conditional probability and conditional expectation, given a set $B$, produced a real numbers.  Now, however, the conditional probability and conditional expectation, given a partition, produce a random variable.

In probability we often work with an abstract probability space that does not encode any information at all.  This author personally believes that a more thorough classroom treatment of the underlying probability spaces used in mathematical proofs would clarify probabilistic exposition.  However, at this point we must remain apathetic to existensial mysteries of $(\Omega, \mcF, \bbP)$.  Instead, we will put structure on this probability space using random variables.  I like to think of this as a statistical construction of a model.  

Random variables encode qualitative information and quantitative information.  Qualitative information comes in the form of logical propositions.  For instance, if $S_n$ is a random variable that represents the number of times a fair coin lands heads up, then $\{S_5 = 2\}$ corresponds to the subset $\{\omega \in \Omega | S_5(\omega) = 2\}$, which corresponds to the proposition $\forall \omega : S_5(\omega) = 2$.  Since we are working within the context of set theory the set described and the logical proposition are the same thing.  We do not care much about the specifics of $\mcF$.  We simply assume that it can support the question ``which $\omega$ satisfy $S_5(\omega) = 2$?'' or any other question we might like to ask for that matter.  Mathematicians call this measurablility.  We can think of measurability as ``the questions we are allowed to ask.''  If $S_5$ is $\mcF-\mcS$ measurable, then we $\mcF$ encodes enough information to answer questions about a coin being flipped five times.  If $S_5$ is not $\mcF-\mcS$ measurable, then $\mcF$ does not encode enough information to ask questions about a coin being flipped five times.

Quantitative information comes in the form of a distribution.  A distribution is a probability measure.  When constructing a statistical model one generally puts some sort of constraints on the distribution of a random variable.  For instace, one may assume that a random variable is normally distributed or that it is exponentially distributed.  Like $\mcF$, the probability $\bbP$ is given meaning within the context of a specific random variable.  If we continue our example above, $S_2$ is the number of times a fair coin lands heads up after two tosses.  Thus the distribution of $S_2$, $\bbP(S_2 = i)$ where $i=0,1,2$ is well defined by the description of the model.  Let's look at conditional probability within the statistical context we are describing above.

For now, let's take $X$ and $Y$ to be two discrete random variables taking values in $(S,\mcS)$.  We want to examine the conditional distribution of $X$ given $Y$.  If we were to condition on a single value of $Y$, for instance $Y = y$, then the conditional distribution would be a number and is well defined by our work above as
\[
\bbP(X \in A | Y = y).
\]
But we want to condition on all of the information present in $Y$ at one time.  To that end, notice that a random variable mapping into the discrete state space $(S,\mcS)$ naturally partitions the sample space $\Omega$.  In fact, the \sigalg generated by $Y$  is equivalent to the partition $\{ Y = y_i : y_i \in S\}$.  The conditional distribution of $X$ given $Y$ is then defined like a conditional distribution on a partition to yield the random variable
\[
\bbP(X \in A | Y)(\omega) = \sum_i \bbP(X \in A | Y = y_i) 1_{\{y_i\}}(Y(\omega)).
\]
There is a convienent alternative to this expression using probability mass functions.  Since $X$ and $Y$ are discrete random variables we may define the joint probability mass function as
\[
p(x,y) = \bbP(X = x, Y = y).
\]
In terms of the underlying probability $\bbP$, the conditional probability mass function is given by
\[
p(x|y) = \bbP(X = x \; | Y = y).
\]
In terms of the joint probability mass function, the conditional probability mass function is given by
\[
p(x|y) = \frac{p(x,y)}{p(y)}.
\]
The pmf is defined for all $y$ in the range of $Y$.  Equivalently, the probability mass function is defined $\bbP^Y$ almost surely, where $\bbP^Y(B)$ is the distribution of $Y$, $\bbP(Y \in B)$.  One simple way to universally define $p(x|y)$ is 
\[
p(x|y) = \frac{p(x,y)}{p(y)} \; \1 {\{\textmd{Support } Y \}}(y).
\]
The indicator function simply sets $p(x|y) = 0$ whenever $p(y)=0$.
We can then write the conditional distribution in several different ways:
\begin{align*}
\bbP(X \in A | Y)(\omega) & = \sum_{x_j \in A} \sum_i p(x_j | y_i) 1_{\{y_i\}}(Y(\omega)) \\
& = \sum_{x_j \in A} p(x_j | Y(\omega)) \\
& = \bbP(X \in A | Y = y) \Big|_{y = Y(\omega)}.
\end{align*}
This equality connects the \emph{random variable} $\bbP(X \in A | Y)$ and the number $\bbP(X \in A | Y = y)$.  In particular, we can compute the distribution of $X$ conditioned on $Y$, which is a random varible, through the distribution of $X$ conditioned on $Y = y$, which is a number.

I claimed earlier that the conditional distribution is a much richer mathematical object than a conditaion expectation, and now we can see why.  Given any Borel measurable function, we can calculate the conditional expectation $\bbE[f(X)|Y]$ using the conditional distribution since
\begin{align*}
\bbE[f(X)|Y] & = \sum_{j} f(x_j) \bbP(X = x_j | Y) \\
& = \sum_{j} f(x_j) p(x_j | Y).
\end{align*}
Thus we can calculate more than just the conditional expectation of $X$.  For instance, we can calculate any moment of $X$ given $Y$, we can calculate the moment generating function of $X$ given $Y$, and we can calculate the characteristic function of $X$ given $Y$.

To this point we had only been working with random varialbes that take on a discrete set of values.  We want to extend these ideas to random variables that map into a continuum.  Before we dive into the mathematics needed to extend these ideas, let's pause and appreciate why we want to use such random varibles in the first place.

Continuous random variables are ontologically similar to differentiable functions.  We use differentiable functions to model discrete phenomenon so that we may use calculus and analysis.  This has been a highly successful endeavor.  A large portion of the physical world is modeled using partial differential equations to provide one example.  However, this does not mean that the universe exists at the level of the continuum.  For instance, the Navier-Stokes partial differential equation models the flow of water.  Yet fluids are made of particles, which are discrete entities.  The model is highly accurate, but it is an approximation of reality.

Continuous random variables are an analogous approximation to reality.  It may be the case that we perform an experiment which provides data up to the hundreths place.  One could try to describe the results of such an experiment using a discrete random variable, but this might be a computational challenge.  It may be more effective to model this experiment using a continuous random variable.  Such a representation provides a good approximation since our instrument has a sufficient level of precision.  In the context of our discussion, we can interpret the conditional distribution $\bbP(X \in A | Y = y)$ as the conditional distribution of $X$ after performing an experiment using an instrument of infinite precision.

Returning to the mathematics, we want to extend the ideas above, which work well for discrete random variables, to random variables which have a density.  There are several different ways we could proceed.  Above we started with the probability $\bbP$ and calculated coditional distributions in terms of probability mass functions.  However, we could have started with probability mass functions and constructed the same theory from that point.  Thinking along these lines, one can start with a probability density function and then define conditional distributiosn and conditional expectations.  If $X$ and $Y$ are two random variables with joint density given by $p(x,y)$, then we can define the conditional density of $X|Y$ as
\[
p(x|y) = \frac{p(x|y)}{p(y)} \; \textmd{ on the support of } Y.
\]
The conditional distribution given $Y=y$ is then given by
\[
\bbP(X \in A | Y = y) = \int_A p(x \; |y) dx
\]
and the conditional distribution given $Y$ is given by
\[
\bbP(X \in A | Y)(\omega) = \int_A p(x \; |Y(\omega)) dx.
\] 
While this approach is computationally convenient I find it philosophically more satisfying to justify these conditional objects through some sort of limiting process.

Suppose we were to start as we did with discrete random variables.  The conditional distribution
\[
\bbP(X \in A | Y \in B) = \frac{\bbP(X \in A, Y \in B)}{\bbP(Y \in B)}
\]
is well defined so long as $\bbP(Y \in B) > 0$.  Thus the intuitive notion of a conditional distribution stays the same.  But we run into trouble when we try to condition on specific values of $Y$,
\[
\bbP(X \in A | Y = y),
\]
since $\bbP(Y = y)$ has zero mass.  If we think of $Y$ as the result of some experiement, then $Y = y$ is a rather unlikely set to encounter since the instrument we use to measure $Y$ is most likely imprecise and noisy.  Hence the most truthful statement we can make about the result of our experiment is probably something like $Y \in B_\ep(y)$ where $\ep$ is a small value.  Furthermore, the event $Y \in B_\ep(y)$ has positive mass and hence the conditional distribution
\[
\bbP(X \in A | Y \in B_\ep(y))
\]
is well defined.  As we shrink $\ep$ to zero this quantity becomes our definition of the conditional distribution of $X$ given $Y = y$, provided the limit exists.  Thus
\[
\bbP(X \in A | Y = y) = \lim_{\ep \searrow 0} \bbP(X \in A | Y \in B_\ep(y)).
\]

Let us return to our definition of $\bbP(X \in A | Y = y)$.  We want to justify that this quantity is well defined.  Writing this limit using our original definition of conditional expectation we have
\[
\bbP(X \in A | Y = y) = \lim_{\ep \searrow 0} \frac{\bbP(X \in A, Y \in B_\ep(y))}{\bbP(Y \in B_\ep(y))}.
\]
To simplify notation, let $Z = \1_A(X)$.  Let us construct two new measures
\[
\nu_A(B) = \bbE[Z 1_B(Y)] \; \textmd{ and } \; \mu(B) = \bbP(Y \in B).
\]
The balls $B_\ep(y)$ shrink nicely to $y$ and $\nu << \mu$, the limit
\[
\lim_{\ep \searrow 0} \; \frac{\nu_A(B_\ep(y))}{\mu(B_\ep(y))}
\]
is well defined according to a variant of Lebesgue's differentiation theorem.  Furthermore, the limit is identical to the Randon-Nikodym derivative $d\nu / d\mu$.  I NEED TO RIGOROUSLY JUSTIFY THE ABOVE STATEMENT.  It will be more convenient in the mathematical proofs that follow to use the Randon-Nikodym derivative $f = d \nu_A / d \mu$ and thus we make the formal definition of the distribution of $X$ conditioned on $Y = y$ to be
\[
\bbP(X \in A | Y = y) = \frac{ d \nu_A }{d \mu}.
\]

% This may not be the problem.
\begin{comment}
However, with both the limiting definition and the Radon-Nikodym derivative we need to make sure that we can recover the joint distribution of $X$ and $Y$ from our conditional distribution.
In particular, we need to show that
\[
\bbP(X \in A, Y \in B) = \int_B \bbP(X \in A | Y = y) \bbP^Y(dy).
\]
Here $\bbP^Y(dy)$ denotes integrating with respect to the distribution of $Y$.  We choose to approach this problem from the perspective of the Radon-Nikodym derivative.

This is a rather delicate point though.  The conditional expectation is defined $\bbP^Y$ almost surely.  When $Y$ is a continuous random variable there could be ``lots'' of points for which $\bbP(X \in A | Y = y)$ is undefined.
\end{comment}

Unfortunately, there is subtle technical point we must overcome in our argument above.  For a fixed $A \in \B(\R)$ the object $\bbP(X \in A | Y = y)$ is defined $\bbP^Y$ almost surely.  To detach ourselves from the current syntax and notation consider a Borel measure $m$.  For $m$ to be a measure it must, at the least, map $\B(\R)$ into $[0,1]$.  This means that $m(A)$ maps into $[0,1]$ for all $A \in \B(\R)$.  However, it is unclear, once we fix $y$, that $\bbP(X \in A | Y = y)$ is well defined for all $A \in \B(\R)$.  For each $A$ there is some null set $N_A$ so that $\bbP(X \in A | Y = y)$ is well defined on $N_A^c$.  Outside of $N_A^c$, $\bbP(X \in A | Y = y)$ could be anything.  Thus we are not guarenteed that $\bbP(X \in A | y = y)$ maps into $[0,1]$.  As a small aside, recall that the limit, as defined above, generates ``the Lebesgue set,'' which is identical to $N_A^c$ up to some measure zero change.

One might suggest at this point to simply set $\bbP(X \in A | Y = y)$ to zero whenever $y \in N_A$.  This ensures that we, at a minimum, have defined a function that maps $\B(\R)$ into $[0,1]$, but this does not make $\bbP(X \in \cdot | Y = y)$ a measure.  For instance, it is possible that there is a disjoint collection of sets $\{A_n\}$ for which $N_{A_n}$ have a common element $y$ and the union of $\{A_n\}$ is all of $\R$.  By the procedure suggested above $\bbP(X \in A_n | Y = y) = 0$ for all $n$, but $\bbP(X \in \cup A_n | Y = y) = 1$.  This violates the conutable additivity required of a measure.

Thus, while defining the conditional distribution in the limiting sense or as a Radon-Nikodym derivative provides the correct computation, it does not provide the machinery of a mathematical proof.  What we need to find is an object $\bbP(X \in A | Y = y)$ so that
\begin{enumerate}
\item $\bbP(X \in \cdot | Y = y)$ is a measure $\bbP^Y$ almost surely, and
\item $\bbP(X \in A | Y = \cdot)$ is a measurable function such that
\[
\bbP(X \in A, Y \in B) = \int_B \bbP(X \in A | Y = y) \bbP(Y \in dy).
\]
This condition is equivalent to
\[
\bbP(X \in A | Y = y) = \lim_{\ep \searrow 0} \frac{\bbP(X \in A | Y \in B_\ep(y))}{\bbP(Y \in B_\ep(y))}
\]
and
\[
\bbP(X \in A | Y = y) = \frac{d \nu_A}{d \mu}.
\]
The aforementioned property shows how one can ``factor out'' the marginal distribution $\bbP(Y \in \cdot)$ from the joint distribution in $X$ and $Y$.
\end{enumerate}

\subsubsection{Regular Conditional Probability}

The object we are looking for is called a \textbf{regular conditional probability} and can be constructed in the following manner.  Note the use of the rational numbers, which area coubtable, dense set within $\R$.  Seperability plays an important role throughout probability.  For instace, one uses similar ideas to move from discrete time to continuous time stochastic processes.

\begin{proof}[Regular Conditional Probability]
For each $q \in \bbQ$, let $F(q,y) = \textmd{RN } \bbP(X \leq q | Y = y)$ where $\textmd{RN } \bbP(\cdots)$ is the Radon-Nikodym derivative as defined above.  For fixed $q$, the function $F(q,y)$ is $\sigma(Y)$ measurable and well defined on $N_q^c$, which is a set of measure unity.  Since the rational numbers are countable $N = \cup_q N_q$ is also a null set.  Then for all $y \in N^c$,
\begin{itemize}
\item $F(q,y)$ is increasing in $q$,
\item $\lim_{q \ra -\infty} F(q,y) = 0$ and $\lim_{q \ra \infty} F(q,y) = 1$.
\end{itemize}
Thus one can define the right continuous extension of $F$ by setting
\[
F(x,y) = \lim_{q \searrow x} F(q,y) \; \textmd{ on } N^c.
\]
The function $F(x,y)$ is a cummulative distribution function for every $y \in N^c$.  Thus one can define a premeasure $\mu_y(-\infty,x] = F(x,y)$, which can be extended to a measure $\mu_y$ through Catheodory's extension theorem.  This is the conditional distribution for which we have been searching.  We formally define the distributionof $X$ conditioned on $Y = y$ as
\[
\bbP(X \in A | Y = y) := \mu_y(A).
\]
Measurability is preserved (something we should prove).  This construction satisfies the factoring formula above since
\begin{align*}
\int_B \bbP(X \leq x | Y = y) \bbP(Y \in dy) 
& = \int_B \frac{d \nu_{(-\infty,x]}}{d \bbP^Y} d \bbP^{Y} \\
& = \bbP(X \leq x, Y \in B). 
\end{align*}
We can then apply the Monotone class theorem to show that this holds for all Borel sets $A \in \B(\R)$, that is
\[
\int_B \bbP(X \in A | Y = y) \bbP(Y \in dy) = \bbP(X \in A, Y \in B).
\]

(I need to show that the RN derivative and the limit defined above do indeed agree with $F(x,y)$. when $x$ is not in $\bbQ$.  I think this will hold by the bounded or dominated convergence theorem.)
\end{proof}

\subsubsection{A look back}

Let us take a moment and reflect upon what we have been doing.  There is one fundamental and invertible operation at work here.  In one sense we have been learning how to factor probability distributions. Let us use a joint density $p(x,y)$ for the moment.  You can factor the joint density $p(x,y)$ as $p(x|y)p(y)$.  However, one can also work in the opposite direction, which is often the case when trying to construct probabilities.  You start with the density $p(y)$ and the conditional density $p(x|y)$ and want to derive the joint density $p(x,y)$.  For instance, this is the case in Bayesian statistics.  Our work with conditional expectation above tells us that the joint density must be $p(x,y) = p(x|y)p(y)$.

So far we have seen several different ways to factor probabilities.  To recapitulate we have
\begin{itemize}
\item Factoring Events: $\bbP(A \cap B) = \bbP(A | B) \bbP(B)$.  

Let us consider a simple example to emphasize that this is not just about factoring, it is also about constructing probabilities.  Suppose we have defined a probability $P_0$ on the \sigalg generated by the disjoint set of events $\{B_i\}$.  Suppose further that for a disjoint set of events $\{A_i\}$ we know what the probability of $A_i$ given an event $B_j$.  Let's denote this quantity by $P_1(A_i | B_j)$.  Then one can define the joint probability of $A_i$ and $B_j$ by $P_2(A_i \cap B_j) = P_1(A_i | B_j) P_0(B_j)$.  The premeasure $P_2$ then generates a measure $\bbP$ defined on the \sigalg generated by the collection $\{A_i,B_j\}$ which is consistent the $P_0, P_1,$ and $P_2$.
\item Factoring densities: $p(x,y) = p(x|y)p(y)$.
\item Factoring a distribution into a conditional distribution and a density:
\[
\bbP(X \in A, Y \in B) = \int_B \bbP(X \in A | Y = y) \bbP(Y \in dy).
\]
\end{itemize}

There is one more type of conditional probability I would like to consider: a probability conditioned on a \sigalg.  Analogous to the properties above, the characteristic property of such a conditional probability is that
\[
\bbP(A \cap B) = \int_B \bbP(A | \mcG) \bbP(d \omega) \textmd{ for all } B \in \mcG.
\]
I think this statement is ultimately somewhat misleading.  We need to be more careful about how we state the above equality.  First, define the measure $Q = \bbP|_\mcG$.  The important point is that $Q$ is a probability defined on $\mcG$.  Second, let $P(A,\omega)$ denote the conditional probability of $A$ given the \sigalg $\mcG$.  This function is analogous to a transition probability or a probability conditioned on $Y=y$.  In particular, $P$ satisfies
\begin{enumerate}
\item  For $\mcG$-almost every $\omega$, $P(\cdot, \omega)$ is a probability on $\mcF$,
\item For every $A \in \mcF$, $P(A,\cdot)$ is $\mcG$-measurable.
\end{enumerate}
Now, with our new variables, we require that the conditional probability $P$ satisfy the factorization
\[
\bbP(A \cap B) = \int_B P(A, \omega) Q(d \omega).
\]
As before, we must justify that such an object, which once more is called a \emph{regular conditional probability}, exists.  We will refrain from presenting the regularity conditions and proof for the moment.

Often when discussing conditional probability or conditional expectation one uses the ubiquitious symbol $\bbP$ throughout.  Whenever you see this, keep in mind that you will be factoring the measure $\bbP$ into a conditional probability and a probability defined \textbf{on a sub-\sigalg}.

\subsection{Conditional Expectation}

The foregoing discussion is an attempt to intuitively build the notion of conditional distribution.  One can calculate conditional expectations using conditional distributions in the usual fashion:
\[
\bbE[X | Y = y] = \int_\R x \bbP(X \in dx | Y = y).
\]

The narrative to this point has suggested we start with conditional distributions and build conditional expectations, but the equality above shows how we could have stared with conditional expectations and derived conditional distributions.  In the end, the technical points we encountered leading up to the regular conditional distribution remain.  Thus one approach is not easier than another, though I personally think the discussion of conditional distributions is both richer and more intuitive than conditional expectation.

We would like extend the notion of conditional distribution and conditional expectation even further.  At this point, the main character in our plot becomes the conditional expectation, though this protagonist can always refer back to his friend the conditional distribution as through the relationship
\[
\bbE[1_A(X) | Y = y] = \bbP(X \in A | Y = y).
\]
Of course, we need a regular conditional probability to makes sense of this and hence there are technical issues we must consider when stating such an equality.

\subsubsection{Calculus of Variations and Conditioning on a \sigalg}

Let us talk about the mean or expectation of a random variable.  Often we think of the mean as the average of a random variable.  This is true in the sense that 
\[
\bbE[X] = \int_\Omega X d \bbP.
\]
The expectation is the weigthed average of $X$ under the measure $\bbP$.  But there are other interpretations of the mean.  

The mean is the best scalar estimate of a random variable under mean square error.  In other words the expected value of $X$ minimizes the functional
\[
J(a) = \int_\Omega (X - a)^2 d \bbP.
\]
If one were to minimize $J(a)$ using standard techniques from calculus he or she would indeed find that
\[
\bbE[X] = \arg \min_a J(a).
\]
I find this interpretation more fundamental than the weighted average of $X$.  This minimization tells us why the weighted average of $X$ matters in the first play.  One can try to find the best scalar estimate of $X$ using other measures of error.  For instance, minimizing
\[
\int_\Omega |X - a| d \bbP
\]
produces the median of $X$ while minimizing
\[
\bbP( |X - a| \geq \ep )
\]
as $\ep$ goes to zero produces the mode of $X$.  (When does one encounter the harmonic mean?  The geometric mean?  What can one say about these quantities?)

We can take this idea and extend it to more complicated objects.  To do so we need to examine the space in which random variable $X$ resides.  For the discussion that follows it will be most helpful to think of $X \in L^2(\Omega, \mcF, \bbP)$.  We will then be working in an inner product space.  Inner product spaces are nice because the have a geometric interpretation.  From this point forth ``best estimate'' will refer to ``best estimate under mean squared error.''  For those familiar with least squares, this suggest that the best estimate is a projection onto a linear subspace.

We can look at estimates for $X$ within linear subspaces of $L^2(\Omega, \mcF, \bbP)$.  For instance, we could try to find the best estimate of $X$ over elements of $span \{Y_0, \cdots, Y_n\}$, which is a linear subspace of $L^2(\Omega, \mcF, \bbP)$.  
More general yet, when $\mcG$ is a sub-\sigalg of $\mcF$, then $L^2(\Omega, \mcG, \bbP)$ is a closed linear subspace of $L^2(\Omega, \mcF, \bbP)$.  Since we are working in a Hilbert (and hence Banach) space we have the calculus of variations at our disposal to minimize functionals.  Thus, the best $\mcG$-measurable estimate of $X$ is the random variable which minimizes the functional
\[
J(Z) = \int_\Omega (X - Z)^2 d \bbP \; \textmd{ over all } \mcG-\textmd{measurable } Z.
\]
This is analogous to our definition of the expectation above, but now we let $Z$ reside in a space larger than $\R$.

Calculating the Gateaux derivative of $J$ (which coincides with the Frechet derivative in this case) we get
\[
DF[Z](V) = 2 \int_\Omega (X-Z) V d \bbP
\]
The first order condition for the minimum value $Z^*$ is then
\[
\int_\Omega (X - Z) V d \bbP = 0 \; \textmd{ for all } \mcG-\textmd{measurable } V.
\]
Since we are working in an inner product space we can refer to our geometric intuition.  For instance, $X-Z^*$ is perpendicular to the subspace $L^2(\Omega, \mcG, \bbP)$, or $Z^*$ is the projection of $X$ onto $L^2(\Omega, \mcG, \bbP)$.

An equivalent requirement for our first order condition to hold is that
\[
\int_\Omega (X - Z) 1_A d \bbP \; \textmd{ for all } A \in \mcG.
\]
This reduces to the familiar definition of conditional expectation.  The conditional expectation of $X$ given $\mcG$ is the $\mcG$-measurable random variable $Z$ for which
\begin{equation}
\label{classic-cond-expectation}
\int_A X = \int_A Z \; \textmd{ for all } A \in \mcG.
\end{equation}
This definition is advantageous because it works for all $X \in L^1(\Omega, \mcF, \bbP)$.  However, I personally think it is less intuitive since one loses geometric intuition when moving beyond inner product spaces.

In deriving (\ref{classic-cond-expectation}) we started with $L^2$, which is an inner product space and hence intuitive.  We defined conditional expectation in this space as the best estimate of a random variable.  This gave us the geometric interpretations of conditional expectation that make sense in an inner product space.  However, to understand what is happening when we move beyond $L^2(\Omega, \mcF, \bbP)$ we must work in the opposite direction.  We now take (\ref{classic-cond-expectation}) as our starting point and work from their to create meaning to the conditional expectation.  To do so we will need to discuss Banach spaces, linear functionals, and duality.

Our starting point, as suggested above, is the linear functional
\[
L_X(\1_A) = \int_\Omega X \1_A \bbP.
\]
The linear functional, as currently defined maps simple functions into real numbers, i.e. $L_X : \{\textmd{simple functions}\} \mapsto \R$.  Simple functions sit within the space of all measurable functions $\L^0(\Omega, \mcF, \bbP)$.  We want to enlarge the domain of $L_X$ to a closed Banach space $D$ so that $L_X : D \mapsto \R$ is continuous.  
This is getting more complicated than I anticipated.  See Ash's Probability and Measure, section 4.2.  One can extend the space on which linear functionals are defined in a meaningful way.  I think this will give us $L^\infty$.  This, in turn, should relate to duality for linear functionals.  This will ultimately mean that the conditional expectation $Z$ of a random variable $X$ is the $\mcG$-measurable random variable such that
\[
\ip{X}{V} = \ip{Z}{V} \textmd{ for all } V \in \textmd{some space}.
\]
Thus one can see that $X$ and $Z$ define the same linear functional.  But I'm not sure what intuition this gives a person about the meaning of conditional expectation.

We can think about the integral above as linear functionals.  If $X \in L^1$, then we can define
\[
L_X(V) = \int_\Omega X V d \bbP
\]
where $V \in L^\infty$.  The linear functional $L_X(V)$ defines a linear functional on $L^\infty(\Omega, \mcG, \bbP)$.  If we are looking for the representative from $L^1(\Omega, \mcG, \bbP)$ then we get $\bbE[X|\mcG]$.


\subsubsection{Bringing it all together}

One big difference between our discussion of conditional distributions and conditional expectations was the type of conditioning occuring.  When we were looking at conditional distributions, we often refered to conditioning on a \sigalg generated by a random variable.  With conditional expectation we refered to conditioning with respect to an arbitrary \sigalg.

(Need to write something connection conditional distribution to conditional expectation.  Perhaps through $\bbE(X|Y) = \bbP(X|Y=y)|y=Y$.)

\chapter{Markov Chains}

We begin this narrative with discrete time stochastic processes.  Psersonally, I have found that discrete time probability provides the intuition necessary to be an effective probabilist.  The theorems one encounters in discrete time stochastic processes almost always carry over to continuous time, but with slight technical modifications.  Thus the first portion of these notes will be devoted to discrete time.  We begin we some preliminary definitions.

Suppose that we have a state space $(S,\mcS)$ and a probability space $(\Omega, \mcF, \bbP)$.  A sequence of $\mcS-\mcF$ measurable random variables $(X_n)_{n \in \bbN}$ is a stochastic process.  Note that we index time in the natural numbers.  We might also index time in the whole numbers $\bbN_0$.  Sometimes we will suppress the subscript.

Statistically speaking, the law of the process is important.  This is a somewhat ambiguous statement.  Generally, the law of the process refers to the distribution generated by the finitie dimensional distributions of the law of the process.  By this I mean that for every finite subsequence $(n_k)$ of the natural numbers we look at the distribution defined by $Q^{(n)}(A_1 \times \cdots \times A_k) = \bbP(X_{n_1} \in A_1, \cdots, X_{n_k} \in A_k)$.  The collection of such distrubtions can be extended to a unique measure $\bbQ$.  This can be thought of the law of the entire process.  This law corresponds to the distribution generated by $\bbP(X \in A)$ where $A$ is a Borel set in the set of functions from $\bbN \times \Omega$ into $\bbR$.

The foregone discussion is still rather ambiguous.  We need to be more specific about the statistical properties we expect our process to possess.  For instance, we might be interested in martingales.  Statistically speaking, a martingale is a process whose forecast any distance into the future, given the entire history of the process, is the current value of the process.  Using mathematical syntax this is saying that $\bbE[X_{n+t} | X_0, \cdots, X_n ] = X_n$.  Another interpretation of this property is that the statistic composed of $X_0$ through $X_n$ that closest to $X_{n+t}$ is identical to $X_n$.  (Really, statistic here is taken in some sort of extended sense, whereby we mean any $\sigma(X_0, \cdots, X_n)$-measurable function.) 

Martingales can be thought of as the fundamental random object within stochastic processes.  All stochastic process can be decomposed into a ``deterministic'' part and a ``random'' part.  The random part is always a martingale.  Martingales can also be thought of as the largest non-parametric collection of stochastic processes (modulo processes of finite variation).  In this way, the collection of martingales is to a parametric family of random process as an $L_1$ function is to the parametric family of normal random variables.

As suggested by the title of this course, we are interested in yet more specific stochastic processes.  In particular, we want to study Markov processes.  Markov processes are a special subset of martingales.  Whereas martingales are defined as a forecasted expected value given the historical trajectory, Markov Processes are defined in terms of future distributions conditioned on past information.  

A \textbf{Markov process} \index{Markov Process} $(X_n)$ is a stochastic process such that the distribution of $X_{n+t}$ conditioned on $X_0, \cdots, X_n$ is identical to the distribution of $X_{n+t}$ conditioned on $X_n$.  In mathematical syntax,
\[
\bbP(X_{n+1} \in B | X_0, \cdots, X_n) = \bbP(X_{n+1} \in B | X_n) \textmd{ for all } B \in \mcB(\R^d).
\]
A Markov process is \index{time homogeneous} \textbf{time homogeneous} when the conditional distribution of $X_{n+1}$ given $X_n$ does not depend on $n$.

The first consequence of this definition is that a Markov process is a martingale as claimed earlier.  In particular, a Markov process is a martingale whose forecast any distance into the future, given \emph{only its current value}, is its current value.  Interpreting this in terms of statistics again, the  statistic composed soley of $X_n$ that is closest to $X_{n+t}$ is identical to $X_n$.  We often say that ``Markov Processes'' have no memory, since the above definitions tell us that
\[
\bbE[X_{n+t} | X_0, \cdots, X_n] = \bbE[X_{n+t} | X_n ].
\]
In other words, all the information encoded in $X_0$ through $X_{n-1}$ is not used; it is forgotten.

\begin{example}[Ehrenfest Chain]
Imagine a box containing $r$ particles is partitioned by a panel with a small opening.  At each step in time, a particle is picked uniformly at random and moved to the opposite side of the box.  Thus if there are $k$ particles in Room 1 at time $n$, and a particle is selected from Room 1, then there will be $k-1$ particles in Room 1 at time $n+1$.  Conversely, if there are $k$ particles in Room 1 at time $n$, and a particle is selected form Room 2, there there will be $k+1$ particles ir Room 1 at time $n+1$.  To put this within a probabilistic framework, let
\[
X_n := \{ \# \textmd{ of particles in Room 1} \}.
\]
Since a particle was selected uniformly at random the  distribution of $X_{n+1}$ given $X_n$ is defined by
\[
\begin{cases}
\bbP(X_{n+1} = k-1 | X_n = k) = k/r \\
\bbP(X_{n+1} = k+1 | X_n = k) = (r-k)/r
\end{cases}.
\]
In this case we construct the probability $\bbP$ so that $X_n$ is a Markov Process under $\bbP$.  Throughout this course we will construct various Markov processes.  Conversely, sometimes we will be given a process and then show that it is Markov.
\end{example}

\begin{example}[Branching Process]
Imagine several cells residing within a nutritious petri dish.  At each step in time a cell may divide into any number of new cells.  It also may not divide at all.  A cell that does not divide is said to divide into one cell.  Each cell divides independently and according to the same law $Q$.  We want to model the population of cells through time.

Let $(\xi_{n,k})$ be a sequence of independent random variables mapping into the natural numbers with distribution $Q$.  Let $X_n$ denote the population of cells at time $n$.  Between time $n$ and $n+1$ the $k$th cell divides into $\xi_{(n+1),k}$ cells.  Thus if we start with $i$ cells at time $n$, we will have $\sum_{k=1}^i \xi_{(n+1),k}$ cells at time $n+1$.  The probability of this transition is given by
\[
\bbP(X_{n+1} = j | X_n = i) = \bbP \Big[ \sum_{k=1}^i \xi_{1,k} = j \Big].
\]
\end{example}

\begin{example}[Flipping an unknown coin]
Suppose we have a coin, but we do not know how the coin is weighted and hence we do not know how often it will lands heads up and how often it will land tails up.  We do know that the coin is not changing significantly through time.  Hence the frequency with which the coin will land heads up does not change in time.  (This is a somewhat paradoxical statement, since frequency is definied as some long term average and hence the coin very well could be changing in time, but we might not see this in the frequency.)  Let $X_n$ represent the $i$th flip of the coin, where $X_i = 1$ if the coin lands heads up and $X_i = 0$ if the coin lands heads down.  Let $\Theta_n$ represent our state of knowledge about the probability of the coin at time $n$.  Then the conditional distribution of $X_i$ given $\Theta_0$ is
\[
\begin{cases}
\bbP[ X_i = 1 | \Theta_0 = \theta] = \theta \\
\bbP[ X_i = 0 | \Theta_0 = \theta] = 1 - \theta.
\end{cases}
\]

As an excercise in Bayesian inference, let's calculate the posterior distribution of $X_{n+1}$ given the data $X_{1}$ through $X_{n}$.  We will do this through using the intermediary $\Theta_n$.  In particular, we know that
\begin{align*}
\bbP [ X_{n+1} = 1 | X_1 = i_1 , \ldots, X_n = i_n ]
& = \int_0^1 \bbP [  X_{n+1} = 1 | \Theta_{n} = \theta , X_1 = i_1 , \ldots, X_n = i_n ] \\
& \;\;\;\; \times \bbP[ \Theta_{n} \in d\theta | X_1 = i_1, \ldots, X_n = i_n] d \theta.
\end{align*}

WHEN DISCUSSING CONDITIONAL EXPECTATION DISCUSS THIS TERMINIOLOGY, NOTATION, AND TRICK.

But once one knows that $\Theta_{n} = \theta$, our previous coin flips provide no useful information.  The the product given above becomes
\[
\bbP[ X_{n+1} = 1 | \Theta_{n} = \theta] \times
\bbP[ \Theta_{n} \in d \theta | X_1 = i_1, \ldots, X_n = i_n].
\]
Thus we have factored the posterior distribution of $X_{n+1}$ given $X_1$ through $X_n$ as the predictive distribution of $X_{n+1}$ given $\Theta_n$ times the posterior distribution for $\Theta_n$ given $X_1$ through $X_n$.  If we assume that our prior distribution, that is our distrubtion for the probability of the coin at time zero is uniform, then we can derive analytically (through conjugacy) that
\[
\bbP[ \Theta_n \in d \theta | X_1 = i_1, \ldots, X_n = i_n]
= \frac{\theta^k (1 - \theta)^{n-k} d \theta }{\int_0^1 \theta^k (1-\theta)^k d\theta}
\]
where $k = i_1 + \cdots i_n$.  Thus we see that $X_1 + \cdots + X_n$ is a sufficient statistic for $\Theta_n$.  In other words, if we let $S_n$ be the number of times heads has been flipped in $n$ trials, $S_n = X_1 + \cdots + X_n$, then we can derive that
\[
\bbP[\Theta_n \in d \theta | S_n = k] 
= \frac{\theta^k (1 - \theta)^{n-k} d \theta }{\int_0^1 \theta^k (1-\theta)^k d\theta}.
\]
Concluding our work above this shows us that
\begin{align*}
\bbP[X_{n+1} = 1 | S_n = k]  
& = \int_0^1 \bbP[X_{n+1} = 1 | \Theta_n = \theta] \times \bbP[ \Theta_n \in d \theta | S_n = k] \\
& = \int_0^1 \underbrace{\theta}_{\textmd{likelihood}} \times \underbrace{\frac{\theta^k (1 - \theta)^{n-k} d \theta }{\int_0^1 \theta^k (1-\theta)^k } \; d \theta}_{\textmd{``posterior''}}
\end{align*}
This shows us how to derive the transition probability for the process $S_n$ since
\[
\bbP[S_{n+1} = k+1 | S_n = k] = \bbP[X_{n+1} = 1 | S_n = k].
\]
This example is a slight twist on the random walk in which one infers the chance a particle moves to the right or left as he or she observes the particle.

\begin{comment}
\begin{remark}
This example also illustrates a fable articulated by Nasim Taleb.  Imagine a turkey that lives happily alone on a farm.  Every day the turkey, who has deep existensial concerns, records whether he lives or does not live through the day.  He is not told with what chance he will live, but he assumes that every day is more or less like the last.  After living for $n$ consecutive days the turkey's posterior probability prediction that he will live tomorrow is given by
\[
\bbP[X_{n+1} = 1 | S_n = n] = \frac{\int_0^1 \theta^{n+1} d \theta}{\int_0^1 \theta^n d \theta} = \frac{n+1}{n+2}. 
\]
Thus the turkey thinks it is very likely that he will live tomorrow. 
...

The more I think about this though, the more Talib doesn't make sense.   For the most part the turkey is correct.  He does have a high chance of living throughout the day.  The day that this does not occur does have a small probabilty of occuring.  It also has severe consequences.  Really, it is the consequences that are devestating.  One must have a loss function to measure how devastating these consequences are.  Relating this to finance, the turkey needs to have some sort of insurance or financial instrument he is using to make this analogy valid.  Even then, if he has calculated his probability correctly, it shouldn't matter.  One wants to include extreme events because they affect the price of an assset.  With CDO's extreme events should have made these assets much cheaper relative to the alleged return.
\end{remark}
\end{comment}

\end{example}

We now formally define a Markov process and transition probability.  As discussed earlier, a Markov process is a process whose future distribution, conditioned on the past observations, depends only on the last observed state of the process.  The conditional distribution, in turn, can be characterized by a transition probability, which tells one how the process will be distributed one step into the future, given the current state of the process.

\begin{definition}
Let $(\Omega, \mcF, (\mcF_n), \bbP)$ be a probabilty space and $(S,\mcS)$ be a state space.  A \index{Markov chain} \textbf{Markov chain} or \index{Markov process} \textbf{Markov process} with respect to the filtration $(\mcF_n)$ is a stochastic process $(X_n)$ whose conditional distribution is governed by
\[
\bbP(X_{n+1} \in B | \mcF_n) = \bbP(X_{n+1} \in B | X_n) \textmd{ for all } B \in \mcS.
\]
When $X_n$ is a Markov chain with respect to some filtration $(\mcF_n)$, it is a Markov chain with respect to the natural filtration as well.

When the conditional distribution $\bbP(X_{n+1} \in \cdot | X_n)$ does not depend upon $n$ the Markov chain is said to be \index{homogeneous Markov chain} homogeneous.
\end{definition}

\begin{definition}
A function $p:S \times \mcS \ra [0,1]$ is a \index{transition probability} \textbf{transition probability} if
\begin{enumerate}
\item for all $x \in S$, $A \mapsto p(x,A)$ is a probability,
\item for all $A \in \mcS$, $x \mapsto p(x,A)$ is measurable.
\end{enumerate}
\end{definition}

A transition probability characterizes a Markov process and a Markov process characterizes a transition probabiltiy.  To clarify this link we must discuss the connection between conditioning on a random variable and conditioning on a \sigalg.

\begin{fact}
A Markov chain has a transition probability $p_n$ from $X_n = x$ to $X_{n+1}$ for all $n$.  Recall that the conditional distribution of $X_{n+1}$ only depends on the previous state $X_n$.  In particular, $\bbP(X_{n+1} \in B | \mcF_n) = \bbP(X_{n+1} \in B | X_n)$.  If $X_n = x$, then we can calculate the transition probability from $x$ to $X_{n+1}$ by
\[
p_n(x,B) = \bbP(X_{n+1} \in B | X_n = x).
\]
We will be working primarily with homogeneous Markov chains, in which case there exists one transition probability $p$ such that
\[
p(x,B) = \bbP(X_{n+1} \in B | X_n = x).
\]
\end{fact}

Transition probabilities provide a convenient way to factor the law of a Markov chain $X = (X_n)$.  The law of a Markov process is defined on the cylinder sets of the state space.  We will discuss the specifics of cylinder sets shortly.  For now, we will simply look at the joint distribution of the first $2$ random variables of $X$.  That is we want to look at 
\[
\bbE[ 1_{B_0}(X_0) 1_{B_1}(X_1) ].
\]
We can factor this expectation as
\[
\bbE \Big[ \; \bbE \Big( \; 1_{B_{1}}(X_{1}) \Big| \mcF_{0} \Big) \; 1_{B_0}(X_0) \; \Big].
\]
Since we have a Markov process this becomes
\[
\bbE \Big[ \; \bbE \Big( \; 1_{B_{1}}(X_{1}) \Big| X_{0} \Big) \; 1_{B_0}(X_0) \; \Big].
\]
Using what we know about conditional expectations when conditioning on $X_0$ get
\[
\bbE \Big[ \int_{B_1} p(X_0, dx_1) \; 1_{B_0}(X_0) \; \Big].
\]
which in turn is equal to
\[
\int_{B_0} \mu(dx_0) \int_{B_1} p_0(x_0, dx_1).
\]
We have writting this integral in ``physics notation'' to keep track of the order of integration.  We integrate along $x_1$ first and then $x_0$.  This holds for higher dimensional joint distributions.  It allows us to derive a factorization the joint distribution of $(X_0, \ldots, X_n)$ as
\[
\bbP(X_0 \in dx_0, \ldots, X_n \in dx_n) = \mu(dx_0) p_0(x_0, dx_1) \cdots p_{n-1}(x_{n-1}, dx_n).
\]

\subsubsection{A note on the state space}

To this point we have been apathetic to the state space $(S, \mcS)$.  At times we do not specify what $(S, \mcS)$ is and at times we suggest that the state space is the real numbers.  This ambiguity is not a hindrance since most state spaces can be mapped into the real numbers in a way that preserves the probabilistic characteristics of the underlying Markov Chain.

\begin{definition}
Suppose $(S,\mcS)$ is a state space.  If there is an measurable, injective function $\phi : S \mapsto \R$ which has a measurable inverse, then $(S,\mcS)$ is said to be a \index{standard Borel state space} \textbf{standard Borel state space}.
\end{definition}

If one has a Markov chain that maps into a stand Borel state space, then he or she can construct a new chain $Y_n = \phi(X_n)$ with what is essentially identical transition probabilities.  For this reason, we can, without loss of much generality, work with Markov chains which map into the real numbers.

\section{From Transition Probability to Law}

The examples given above were a bit flippant about the mathematical construction of Markov Processes.  In general, one has an idea about how a Markov process should evolve as described by some transition probability.  However, it will require some mathematical rigor to translate a transition probability into a well-defined stochastic process.  For instance, it is not obvoius how one should construct a probability space on which an infinite collection of random variables are defined, nor is it obvious how one defines a law on such a space.  However, this work is well worth the effort, since it will be used repeatedly in the future.  For instance, when one constructs Brownian motion one must use the techniques presented bolow.  This construction will also prove useful when proving properties, such as the Strong Markov property for stocastic processes.

\end{document}
